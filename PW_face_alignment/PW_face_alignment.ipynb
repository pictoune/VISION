{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85f97cb",
   "metadata": {},
   "source": [
    "<font size=\"6\">**Nathan GALMICHE and Victor PIRIOU**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db56d1",
   "metadata": {},
   "source": [
    "<font size=\"5\">**1.2 Data visualisation**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from skimage.transform import resize\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b2a06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_paths(file_name):\n",
    "    with open(file_name) as f:\n",
    "        return np.array([line.rstrip(\"\\n\") for line in f])\n",
    "\n",
    "\n",
    "train_img_path = read_paths(\"data/300w_train_images.txt\")\n",
    "train_landmark_paths = read_paths(\"data/300w_train_landmarks.txt\")\n",
    "test_img_path = read_paths(\"data/helen_testset.txt\")\n",
    "test_landmark_paths = read_paths(\"data/helen_testset_landmarks.txt\")\n",
    "\n",
    "fig = plt.figure(figsize=(15, 20))\n",
    "selected_indices = np.random.choice(len(train_img_path), 12)\n",
    "\n",
    "for i, line in enumerate(train_img_path[selected_indices]):\n",
    "    pts = np.loadtxt(\"data/\" + train_landmark_paths[selected_indices[i]])\n",
    "\n",
    "    try:\n",
    "        img = mpimg.imread(\"data/\" + line)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i + 1)\n",
    "    ax.scatter(pts[:, 0], pts[:, 1], linewidths=0.5)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20805438",
   "metadata": {},
   "source": [
    "<font size=\"5\">**1.3 Data augmentation**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_selected_img = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b184f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell doesn't need to be re-executed\n",
    "\n",
    "# Check and create directory before the loop\n",
    "if not os.path.exists(\"data/helen/preprocessed_train_img\"):\n",
    "    os.mkdir(\"data/helen/preprocessed_train_img\")\n",
    "\n",
    "for i, landmark in enumerate(train_landmark_paths):\n",
    "    if i == nb_selected_img:\n",
    "        break\n",
    "\n",
    "    print(f\"\\r{i+1}/{min(len(train_landmark_paths), nb_selected_img)}\", end=\"\")\n",
    "\n",
    "    img = mpimg.imread(\"data/\" + train_img_path[i])\n",
    "    pts = np.loadtxt(\"data/\" + landmark)\n",
    "    xmin, xmax = pts[:, 0].min(), pts[:, 0].max()\n",
    "    ymin, ymax = pts[:, 1].min(), pts[:, 1].max()\n",
    "    w, h = xmax - xmin, ymax - ymin\n",
    "\n",
    "    # Widening of the bounding box with simplified boundary checks\n",
    "    xmin_exp = max(int(xmin - w * 0.15), 0)\n",
    "    xmax_exp = min(int(xmax + w * 0.15), len(img[0]) - 1)\n",
    "    ymin_exp = max(int(ymin - h * 0.15), 0)\n",
    "    ymax_exp = min(int(ymax + h * 0.15), len(img) - 1)\n",
    "\n",
    "    img = img[ymin_exp:ymax_exp, xmin_exp:xmax_exp]\n",
    "    h, w = img.shape[:2]\n",
    "    img = resize(img, (128, 128))\n",
    "\n",
    "    # Simplified newpts calculation using list comprehension\n",
    "    newpts = [\n",
    "        f\"{(pt[0] - xmin_exp) / w * 128} {(pt[1] - ymin_exp) / h * 128}\" for pt in pts\n",
    "    ]\n",
    "\n",
    "    with open(f\"data/helen/preprocessed_train_img/figure{i}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(newpts))\n",
    "\n",
    "    plt.imsave(f\"data/helen/preprocessed_train_img/figure{i}.png\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc232210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell doesn't need to be re-executed\n",
    "\n",
    "for i, landmark in enumerate(test_landmark_paths):\n",
    "    print(f\"\\r{i+1}/{len(test_landmark_paths)}\", end=\"\")\n",
    "\n",
    "    img = mpimg.imread(\"data/\" + test_img_path[i])\n",
    "    pts = np.loadtxt(\"data/\" + landmark)\n",
    "    xmin, xmax = pts[:, 0].min(), pts[:, 0].max()\n",
    "    ymin, ymax = pts[:, 1].min(), pts[:, 1].max()\n",
    "    w, h = xmax - xmin, ymax - ymin\n",
    "\n",
    "    # widening of the bounding box\n",
    "    xmin_exp = max(int(xmin - w * 0.15), 0)\n",
    "    xmax_exp = min(int(xmax + w * 0.15), img.shape[1] - 1)\n",
    "    ymin_exp = max(int(ymin - h * 0.15), 0)\n",
    "    ymax_exp = min(int(ymax + h * 0.15), img.shape[0] - 1)\n",
    "\n",
    "    img = img[ymin_exp:ymax_exp, xmin_exp:xmax_exp]\n",
    "    h, w = img.shape[:2]\n",
    "    img = resize(img, (128, 128))\n",
    "\n",
    "    newpts = [\n",
    "        f\"{(pt[0] - xmin_exp) / w * 128} {(pt[1] - ymin_exp) / h * 128}\" for pt in pts\n",
    "    ]\n",
    "\n",
    "    os.makedirs(\"data/helen/preprocessed_test_img\", exist_ok=True)\n",
    "    with open(f\"data/helen/preprocessed_test_img/figure{i}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(newpts))\n",
    "\n",
    "    plt.imsave(f\"data/helen/preprocessed_test_img/figure{i}.png\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb00e55",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Check that the preprocess went well:**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6551c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize=(15, 20))\n",
    "\n",
    "selected_indices = np.random.choice(nb_selected_img, 12)\n",
    "\n",
    "for cpt, i in enumerate(selected_indices):\n",
    "    pts = np.loadtxt(f\"data/helen/preprocessed_train_img/figure{i}.txt\")\n",
    "    img = mpimg.imread(f\"data/helen/preprocessed_train_img/figure{i}.png\")\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, cpt + 1)\n",
    "\n",
    "    ax.scatter(pts[:, 0], pts[:, 1], linewidths=0.5)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd438d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell doesn't need to be re-executed\n",
    "\n",
    "fig = figure(figsize=(15, 20))\n",
    "\n",
    "selected_indices = np.random.choice(len(test_landmark_paths), 12)\n",
    "\n",
    "cpt = 0\n",
    "\n",
    "for i in selected_indices:\n",
    "    pts = np.loadtxt(\"data/helen/preprocessed_test_img/figure\" + str(i) + \".txt\")\n",
    "    img = mpimg.imread(\"data/helen/preprocessed_test_img/figure\" + str(i) + \".png\")\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, cpt + 1)\n",
    "\n",
    "    ax.scatter(pts[:, 0], pts[:, 1], linewidths=0.5)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    cpt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_landmarks(dataset_type, landmark_paths, nb_selected_img):\n",
    "    file_path = \"data/helen/preprocessed_{}_img/figure\".format(dataset_type)\n",
    "    mean = np.loadtxt(file_path + \"0.txt\")\n",
    "    limit = (\n",
    "        min(len(landmark_paths), nb_selected_img)\n",
    "        if dataset_type == \"train\"\n",
    "        else len(landmark_paths)\n",
    "    )\n",
    "\n",
    "    for i in range(1, limit):\n",
    "        print(\"\\r{}/{}\".format(i, limit), end=\"\")\n",
    "        pts = np.loadtxt(file_path + str(i) + \".txt\")\n",
    "        mean += pts\n",
    "\n",
    "    mean /= limit\n",
    "    return mean\n",
    "\n",
    "\n",
    "def save_or_load_landmarks(dataset_type, landmark_paths, nb_selected_img):\n",
    "    os.makedirs(\"backup\", exist_ok=True)\n",
    "    file_name = \"backup/mean_{}_landmarks.pkl\".format(dataset_type)\n",
    "    if not os.path.exists(file_name):\n",
    "        mean_landmarks = compute_mean_landmarks(\n",
    "            dataset_type, landmark_paths, nb_selected_img\n",
    "        )\n",
    "        with open(file_name, \"wb\") as file:\n",
    "            pickle.dump(mean_landmarks, file)\n",
    "    else:\n",
    "        with open(file_name, \"rb\") as file:\n",
    "            mean_landmarks = pickle.load(file)\n",
    "    return mean_landmarks\n",
    "\n",
    "\n",
    "print(\"train:\")\n",
    "mean_train_landmarks = save_or_load_landmarks(\n",
    "    \"train\", train_landmark_paths, nb_selected_img\n",
    ")\n",
    "\n",
    "print(\"\\n\\ntest:\")\n",
    "mean_test_landmarks = save_or_load_landmarks(\n",
    "    \"test\", test_landmark_paths, nb_selected_img\n",
    ")\n",
    "\n",
    "\n",
    "def plot_landmarks(image_path, landmarks):\n",
    "    plt.imshow(mpimg.imread(image_path))\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_landmarks(\"data/helen/preprocessed_train_img/figure0.png\", mean_train_landmarks)\n",
    "plot_landmarks(\"data/helen/preprocessed_test_img/figure0.png\", mean_test_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f784e",
   "metadata": {},
   "source": [
    "<font size=\"3\">**5. Why do we generate these perturbations? How could they be estimated automatically?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c637a3",
   "metadata": {},
   "source": [
    "We generate these perturbations to match the variety of faces we can encounter. Also, pictures of faces can be taken from more or less far and with varying angles. The perturbations are data augmentations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell doesn't need to be re-executed\n",
    "\n",
    "num_images = min(len(train_landmark_paths), nb_selected_img)\n",
    "for i in range(num_images):\n",
    "    print(f\"\\r{i+1}/{num_images}\", end=\"\")\n",
    "\n",
    "    img = mpimg.imread(f\"data/helen/preprocessed_train_img/figure{i}.png\")\n",
    "    for j in range(10):\n",
    "        newPts = mean_train_landmarks.copy()\n",
    "\n",
    "        tx = np.random.randint(-20, 20)\n",
    "        ty = np.random.randint(-20, 20)\n",
    "        sx = np.random.randint(-20, 20) / 100\n",
    "        sy = np.random.randint(-20, 20) / 100\n",
    "\n",
    "        perturb = []\n",
    "        for pt in newPts:\n",
    "            pt[0] = min(max(pt[0] * (1 + sx) + tx, 0), 128)\n",
    "            pt[1] = min(max(pt[1] * (1 + sy) + ty, 0), 128)\n",
    "            perturb.append(f\"{pt[0]} {pt[1]}\")\n",
    "\n",
    "        perturb_str = \"\\n\".join(perturb)\n",
    "        with open(f\"data/helen/preprocessed_train_img/figure{i}_{j}.txt\", \"w\") as f:\n",
    "            f.write(perturb_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19826477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell doesn't need to be re-executed\n",
    "\n",
    "plt.imshow(mpimg.imread(\"data/helen/preprocessed_train_img/figure498.png\"))\n",
    "pts = np.loadtxt(\"data/helen/preprocessed_train_img/figure498_6.txt\")\n",
    "plt.scatter(pts[:, 0], pts[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f0bdc",
   "metadata": {},
   "source": [
    "<font size=\"5\">**2.1   Feature extraction**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c50b1",
   "metadata": {},
   "source": [
    "<font size=\"3\">**1. Why do we not directly use the raw value of the image pixels as a representation ?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67d9c3",
   "metadata": {},
   "source": [
    "Because we want a representation that is robust to small changes (e.g. small rotations) and perturbations (e.g. noise) applied on the image, SIFT descriptors are well suited. Indeed they are robust to translation, small rotation, illumination change and scale changes. They allow a spatial encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db65628",
   "metadata": {},
   "source": [
    "<font size=\"3\">**3. What is the dimension of each feature?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940e893",
   "metadata": {},
   "source": [
    "Each patch image is decomposed into 16 sub-regions of size $4 \\times 4$ pixels. Each sub-region is encoded as a vector of size 8. Hence, the dimension of each feature is $4 \\times 4 \\times 8 = 128$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bd139",
   "metadata": {},
   "source": [
    "<font size=\"3\">**4. For each image, concatenate all the computed features from each landmarks. What is the\n",
    "dimension of this feature vector?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608233ec",
   "metadata": {},
   "source": [
    "We have 68 landmarks per image the concatenation of all the computed features results in vector of size $68 \\times 128 = 8704$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(image, keyPoints):\n",
    "    sift = cv2.SIFT_create()\n",
    "    _, descriptors = sift.compute(image, keyPoints)\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def process_landmarks(landmarks, paths, size, prefix, output_file):\n",
    "    keyPoints = [cv2.KeyPoint(x, y, size) for x, y in landmarks]\n",
    "    descriptors_list = []\n",
    "\n",
    "    for i, path in enumerate(paths):\n",
    "        print(f\"\\r{i+1}/{len(paths)}\", end=\"\")\n",
    "        img = cv2.imread(f\"{prefix}{path}.png\")\n",
    "        desc = get_descriptors(img, keyPoints).flatten()\n",
    "        descriptors_list.append(desc)\n",
    "\n",
    "    X = np.column_stack(descriptors_list)\n",
    "    with open(output_file, \"wb\") as file:\n",
    "        pickle.dump(X, file)\n",
    "    return X\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "print(\"train:\")\n",
    "if os.path.exists(\"backup/X0_train.pkl\"):\n",
    "    with open(\"backup/X0_train.pkl\", \"rb\") as file:\n",
    "        X0_train = pickle.load(file)\n",
    "else:\n",
    "    X0_train = process_landmarks(\n",
    "        mean_train_landmarks,\n",
    "        [f\"figure{i}\" for i in range(min(len(train_landmark_paths), nb_selected_img))],\n",
    "        20,\n",
    "        \"data/helen/preprocessed_train_img/\",\n",
    "        \"backup/X0_train.pkl\",\n",
    "    )\n",
    "\n",
    "print(\"\\n\\nX0_train.shape =\", X0_train.shape)\n",
    "\n",
    "# TEST\n",
    "print(\"\\ntest:\")\n",
    "if os.path.exists(\"backup/X0_test.pkl\"):\n",
    "    with open(\"backup/X0_test.pkl\", \"rb\") as file:\n",
    "        X0_test = pickle.load(file)\n",
    "else:\n",
    "    X0_test = process_landmarks(\n",
    "        mean_test_landmarks,\n",
    "        [f\"figure{i}\" for i in range(len(test_landmark_paths))],\n",
    "        16,\n",
    "        \"data/helen/preprocessed_test_img/\",\n",
    "        \"backup/X0_test.pkl\",\n",
    "    )\n",
    "\n",
    "print(\"\\n\\nX0_test.shape =\", X0_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b8f3c",
   "metadata": {},
   "source": [
    "<font size=\"5\">**2.2   Dimensionality reduction**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b247f",
   "metadata": {},
   "source": [
    "<font size=\"3\">**1.What is the main interest of this dimensionality reduction? Could you cite some other\n",
    "dimensionality reduction methods for machine learning?**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3577d",
   "metadata": {},
   "source": [
    "The main interest of this dimensionality reduction is to reduce the computational cost of the approach.  \n",
    "  \n",
    "  However, when applying PCA we have to keep in mind that this method has strong assumptions. Indeed, it assumes that the features can be expressed by a linear combination of a few principal components that are orthogobal.  \n",
    "  \n",
    "Other dimensionality reduction methods can be decomposed in two mains categories :\n",
    "1. **Methods that find a subset of the features.** These include *Forward selection, Backward elimination, Random Forests, etc.*\n",
    "2. **Methods that project the features in a lower dimensional space.** This category can be decomposed into linear and non-linear methods. Linear methods include *Factor Analysis, Linear Discriminant Analysis, Truncated Singular Value Decomposition, etc.* Non linear methods include *Kernel PCA, t-distributed Stochastic Neighbor Embedding, Multidimensional Scaling and Isometric mapping.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b534a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_compute_pca(data_type, data):\n",
    "    prefix = f\"backup/{data_type}\"\n",
    "    files = [\n",
    "        f\"{prefix}_mean.pkl\",\n",
    "        f\"{prefix}_eigenvectors.pkl\",\n",
    "        f\"{prefix}_eigenvalues.pkl\",\n",
    "    ]\n",
    "\n",
    "    if all(os.path.exists(file) for file in files):\n",
    "        with open(files[0], \"rb\") as file:\n",
    "            mean = pickle.load(file)\n",
    "        with open(files[1], \"rb\") as file:\n",
    "            eigenvectors = pickle.load(file)\n",
    "        with open(files[2], \"rb\") as file:\n",
    "            eigenvalues = pickle.load(file)\n",
    "    else:\n",
    "        mean = np.empty((0))\n",
    "        mean, eigenvectors, eigenvalues = cv2.PCACompute2(\n",
    "            data.transpose(), mean, retainedVariance=0.98\n",
    "        )\n",
    "        with open(files[0], \"wb\") as file:\n",
    "            pickle.dump(mean, file)\n",
    "        with open(files[1], \"wb\") as file:\n",
    "            pickle.dump(eigenvectors, file)\n",
    "        with open(files[2], \"wb\") as file:\n",
    "            pickle.dump(eigenvalues, file)\n",
    "\n",
    "    return mean, eigenvectors, eigenvalues\n",
    "\n",
    "\n",
    "# Process both train and test data\n",
    "print(f\"\\n{'train'}:\")\n",
    "train_mean, train_eigenvectors, train_eigenvalues = load_or_compute_pca(\n",
    "    \"train\", X0_train\n",
    ")\n",
    "print(\"done\")\n",
    "\n",
    "print(f\"\\n{'test'}:\")\n",
    "test_mean, test_eigenvectors, test_eigenvalues = load_or_compute_pca(\"test\", X0_test)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c42745",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_tilde = np.matmul(train_eigenvectors, X0_train)\n",
    "print(\"X0_tilde.shape:\", X0_tilde.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a373968",
   "metadata": {},
   "source": [
    "<font size=\"3\">**3. What are the dimensions of the new resulting matrix $\\tilde{\\mathbf{X}}_0$ ?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f1c15",
   "metadata": {},
   "source": [
    "We have $\\tilde{\\mathbf{X}}_0=\\mathbf{A}_0 \\mathbf{X}_0$ where $\\mathbf{A}_0 \\in \\mathbb{R}^{M^{\\prime} \\times M}$ and $\\mathbf{X}_0 \\in \\mathbb{R}^{M \\times N}$ so $\\tilde{\\mathbf{X}}_0 \\in \\mathbb{R}^{M^{\\prime} \\times N}$.\n",
    "\n",
    "Here $M^{\\prime} = 218 $ is the smallest number of principal components that explained at least $98\\%$ of the variance of the original data while $N = 1000$ is the number of images we chose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee2400",
   "metadata": {},
   "source": [
    "<font size=\"5\">**2.3   Displacement estimation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89ebd36",
   "metadata": {},
   "source": [
    "<font size=\"3\">**2. Compute the prediction error (mean absolute error) on the training set.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose to not take into account the augmented data to make PCA computationnally tractable with our computers\n",
    "\n",
    "backup_path = os.path.join(\"backup\", \"points_train.pkl\")\n",
    "if os.path.exists(backup_path):\n",
    "    with open(backup_path, \"rb\") as file:\n",
    "        points_train = pickle.load(file)\n",
    "else:\n",
    "    points_train = np.empty((136, 0))\n",
    "    num_images = min(len(train_landmark_paths), nb_selected_img)\n",
    "    for i in range(num_images):\n",
    "        print(f\"\\r{i}/{num_images}\", end=\"\")\n",
    "        pts = np.loadtxt(f\"data/helen/preprocessed_train_img/figure{i}.txt\").reshape(\n",
    "            (136, 1)\n",
    "        )\n",
    "        points_train = np.append(points_train, pts, axis=1)\n",
    "\n",
    "    with open(backup_path, \"wb\") as file:\n",
    "        pickle.dump(points_train, file)\n",
    "\n",
    "print(\"\\ndone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_landmarks_ = np.tile(\n",
    "    mean_train_landmarks.copy().reshape((136, 1)), (1, X0_tilde.shape[1])\n",
    ")\n",
    "\n",
    "X0_tilde = np.concatenate([X0_tilde, np.ones((1, X0_tilde.shape[1]))], axis=0)\n",
    "\n",
    "delta = points_train - mean_train_landmarks_\n",
    "\n",
    "reg = LinearRegression().fit(X0_tilde.T, delta.T)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08233941",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = reg.predict(X0_tilde.T).T\n",
    "print(\n",
    "    f\"'mean absolute error' sur l'ensemble d'apprentissage = {np.mean(np.abs(delta - train_pred))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6166ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"score sur l'ensemble d'apprentissage = {reg.score(X0_tilde.T, delta.T)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b44e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = train_pred.reshape((-1, 68, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c2476",
   "metadata": {},
   "source": [
    "<font size=\"3\">**2. Display $s_0$ the\n",
    "initial position of the model (in red) and $s_0 + \\delta_s$ the displaced landmarks (in green) for\n",
    "the first image of the training set.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168cd221",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mpimg.imread(\"data/helen/preprocessed_train_img/figure0.png\"))\n",
    "plt.scatter(\n",
    "    mean_train_landmarks[:, 0], mean_train_landmarks[:, 1], color=\"red\", linewidths=0.5\n",
    ")\n",
    "plt.scatter(\n",
    "    mean_train_landmarks[:, 0] + train_pred[0, :, 0],\n",
    "    mean_train_landmarks[:, 1] + train_pred[0, :, 1],\n",
    "    color=\"green\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e656f0",
   "metadata": {},
   "source": [
    "<font size=\"3\">**2. What can we conclude ?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c19718",
   "metadata": {},
   "source": [
    "The results are not good. It would eventually work better with another size of patch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14f833",
   "metadata": {},
   "source": [
    "<font size=\"3\">**3. Why this prediction error is not relevant to evaluate our methods ?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbeac48",
   "metadata": {},
   "source": [
    "This prediction is not relevant to evaluate our methods because it is biased. Indeed, it is computed from an image that was used to train the model. We have to use a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4358bfe0",
   "metadata": {},
   "source": [
    "<font size=\"3\">**4. Compute the prediction error on the test set and display s0 the initial position of the\n",
    "model (in red) and s0 + δs the displaced landmarks (in green) for the 5 first images of the\n",
    "test set.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea96235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose to not take into account the augmented data to make PCA computationnally tractable with our computersif(os.path.exists('backup/points_test.pkl')):\n",
    "if os.path.exists(\"backup/points_test.pkl\"):\n",
    "    with open(\"backup/points_test.pkl\", \"rb\") as file:\n",
    "        points_test = pickle.load(file)\n",
    "else:\n",
    "    points_test = np.array(\n",
    "        [\n",
    "            np.loadtxt(f\"data/helen/preprocessed_test_img/figure{i}.txt\").reshape(\n",
    "                (136, 1)\n",
    "            )\n",
    "            for i in range(len(test_landmark_paths))\n",
    "        ]\n",
    "    ).reshape(136, -1)\n",
    "\n",
    "    with open(\"backup/points_test.pkl\", \"wb\") as file:\n",
    "        pickle.dump(points_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_tilde_test = np.matmul(test_eigenvectors, X0_test)\n",
    "\n",
    "mean_test_landmarks_ = np.repeat(\n",
    "    mean_test_landmarks.reshape((136, 1)), X0_tilde_test.shape[1], axis=1\n",
    ")\n",
    "\n",
    "X0_tilde_test = np.concatenate(\n",
    "    [X0_tilde_test, np.ones((1, X0_tilde_test.shape[1]))], axis=0\n",
    ")\n",
    "\n",
    "delta_test = points_test - mean_test_landmarks_\n",
    "\n",
    "reg_test = LinearRegression().fit(X0_tilde_test.T, delta_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba956d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(delta_test - reg_test.predict(X0_tilde_test.T).T))\n",
    "print(f\"mean absolute error on the train set = {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38841aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = reg_test.score(X0_tilde_test.T, delta_test.T)\n",
    "print(f\"score on the test set = {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.imshow(\n",
    "        mpimg.imread(\"data/helen/preprocessed_test_img/figure\" + str(i) + \".png\")\n",
    "    )\n",
    "    plt.scatter(\n",
    "        mean_train_landmarks[:, 0],\n",
    "        mean_train_landmarks[:, 1],\n",
    "        color=\"red\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    plt.scatter(\n",
    "        mean_train_landmarks[:, 0] + train_pred[i, :, 0],\n",
    "        mean_train_landmarks[:, 1] + train_pred[i, :, 1],\n",
    "        color=\"green\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf6947",
   "metadata": {},
   "source": [
    "<font size=\"3\">**4. What can we conclude ?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef62324",
   "metadata": {},
   "source": [
    "The results are not good. It would probably work better with more data. In order to improve the performance we could also fully implement the cascade regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee5cfb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
